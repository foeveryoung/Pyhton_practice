{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/longbro/Downloads/小贞毕设/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取csv\n",
    "cost = spark.read.format('csv').load(path + 'cost.csv',header = True)\n",
    "trans = spark.read.csv(path + 'trans.csv',header = True)\n",
    "product = spark.read.format('csv').load(path + 'product.csv',header = True)\n",
    "customer = spark.read.csv(path + 'customer.csv',header = True)\n",
    "# 列名重命名1\n",
    "oldColumns  = trans.columns\n",
    "newColumns = ['account','trans_id','trans_name','trans_date','prod_id','prod_name','trans_way','amount']\n",
    "trans = reduce(lambda trans, idx : trans.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)),trans)\n",
    "# 列名重命名2\n",
    "from pyspark.sql.functions import *\n",
    "cost = cost.select(col(\"客户号\").alias(\"account\"),col(\"产品代码\").alias(\"prod_id\"),col(\"投资成本\").alias(\"amount\"))\n",
    "\n",
    "# oldColumns2 = customer.columns\n",
    "# newColumns2 = ['account','cus_type','cus_group','certification','gender','risk','birthday','manager']\n",
    "# customer = reduce(lambda customer,idx: customer.withColumnRenamed(oldColumns2[idx],newColumns2[idx]),\\\n",
    "#                  xrange(len(oldColumns2)),customer)\n",
    "# 列重命名 3\n",
    "customer = customer.toDF('account','cus_type','cus_group','certification','gender','risk','birthday','manager')\n",
    "\n",
    "oldColumns3 = product.columns\n",
    "newColumns3 = [x.strip() for x in product.columns]\n",
    "product = reduce(lambda p, idx:p.withColumnRenamed(oldColumns3[idx], newColumns3[idx]),\\\n",
    "                xrange(len(oldColumns3)),product)\n",
    "\n",
    "# 注册为table1\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "sqlCtx.registerDataFrameAsTable(cost,'cost')\n",
    "\n",
    "# spark.registerTempTable(trans,'trans')\n",
    "# sqlCtx.dropTempTable('trans')\n",
    "\n",
    "# 注册为table2\n",
    "trans.registerTempTable('trans')\n",
    "product.registerTempTable('product')\n",
    "customer.registerTempTable('customer')\n",
    "# spark.registerDataFrameAsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000132</td>\n",
       "      <td>942</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000132</td>\n",
       "      <td>1180</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000132</td>\n",
       "      <td>823</td>\n",
       "      <td>47000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account  prod_id  amount\n",
       "0  1000000132      942    4500\n",
       "1  1000000132     1180    4500\n",
       "2  1000000132      823   47000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cost = pd.read_csv(path + 'cost.csv')\n",
    "cost.columns = ['account','prod_id','amount']\n",
    "# pandas 和 pyspark的 df 转换\n",
    "cost = spark.createDataFrame(cost)\n",
    "cost.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个新的session， 内存和table是共享的，其余是各自的\n",
    "spark2 = spark.newSession()\n",
    "# spark2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-807c5445947e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mamount_div_10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msqlCtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'myudf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount_div_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select account, myudf(amount) from cost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mregisterFunction\u001b[0;34m(self, name, f, returnType)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringLengthInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/catalog.pyc\u001b[0m in \u001b[0;36mregisterFunction\u001b[0;34m(self, name, f, returnType)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserDefinedFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\""
     ]
    }
   ],
   "source": [
    "# range\n",
    "spark.range(1,6,2).toPandas()\n",
    "\n",
    "# sql\n",
    "spark.sql(\"select * from cost limit 5\").toPandas()\n",
    "\n",
    "# 返回一个table为df\n",
    "spark.table('cost').toPandas().head(3)\n",
    "\n",
    "# udf\n",
    "from pyspark.sql.types import *\n",
    "def amount_div_10(x):\n",
    "    return x / 10\n",
    "sqlCtx.registerFunction('myudf', amount_div_10,DoubleType())\n",
    "spark.sql(\"select account, myudf(amount) from cost\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9362\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>avg_amount</th>\n",
       "      <th>buy_times</th>\n",
       "      <th>buy_types</th>\n",
       "      <th>trans_ways</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000599845</td>\n",
       "      <td>15073137</td>\n",
       "      <td>3,014,627</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001099733</td>\n",
       "      <td>6810000</td>\n",
       "      <td>6,810,000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002856447</td>\n",
       "      <td>6180000</td>\n",
       "      <td>2,060,000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account  total_amount avg_amount  buy_times  buy_types  trans_ways\n",
       "0  1000599845      15073137  3,014,627          5          2           1\n",
       "1  1001099733       6810000  6,810,000          1          1           1\n",
       "2  1002856447       6180000  2,060,000          3          1           2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "import pyspark.sql.functions as func\n",
    "# groupby + agg聚合函数\n",
    "# alias 别名、重命名\n",
    "# (func.col(\"trans_way\") == '网上银行') & \n",
    "judge = (func.col(\"amount\")>0) \n",
    "# filter  =  where\n",
    "trans = trans.filter(judge)\n",
    "account_amount = trans.groupBy('account').agg(\\\n",
    "                    sum('amount').alias(\"total_amount\"), \\\n",
    "                    func.format_number(func.avg('amount'),0).alias(\"avg_amount\"), \\\n",
    "                    func.count('prod_id').alias(\"buy_times\"),\\\n",
    "                    func.countDistinct(\"prod_id\").alias(\"buy_types\"),\\\n",
    "                    func.countDistinct('trans_way').alias(\"trans_ways\"))\\\n",
    "                    .orderBy(['total_amount', 'avg_amount'],ascending = [False,True])\n",
    "print account_amount.count()\n",
    "account_amount.toPandas().head(3)\n",
    "# ?func.format_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view 操作\n",
    "cost.createTempView('cost_try')\n",
    "cost = spark.sql('select * from cost_try limit 5')\n",
    "cost.createOrReplaceTempView(\"cost_try\")\n",
    "spark.catalog.dropTempView(\"cost_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+-------------+------+----+--------+-------+-----------+-----+-----+----+\n",
      "|   account|cus_type|cus_group|certification|gender|risk|birthday|manager|      birth|years|month| age|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----------+-----+-----+----+\n",
      "|1000162168|      个人|     普通客户|          身份证|     女| 保守型|19560919|6000493|1.9560929E7| 1956|    9|61.0|\n",
      "|1000162203|      个人|     普通客户|          身份证|     男| 保守型|19820630|       | 1.982064E7| 1982|    6|35.0|\n",
      "|1000190754|      个人|     普通客户|          身份证|     女| 保守型|19551211|       |1.9551221E7| 1955|   12|62.0|\n",
      "|1000190759|      个人|     普通客户|          身份证|     男| 保守型|19550529|       |1.9550539E7| 1955|    5|62.0|\n",
      "|1000190779|      个人|     普通客户|          身份证|     女| 保守型|19831222|       |1.9831232E7| 1983|   12|34.0|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----------+-----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### from pyspark.sql.types import IntegerType\n",
    "# 这种方式没法修改到schema\n",
    "# customer.select(customer.account.astype(IntegerType()))\n",
    "# 对列进行类型转换\n",
    "customer = customer.withColumn('year',substring('birthday',0,4).cast('int')).withColumn('month',substring('birthday',5,2).cast('int'))\\\n",
    "        .withColumn('age',(2017 - col('year')).cast('double')).withColumnRenamed('year','years')\n",
    "#             .withColumn('birthday',unix_timestamp(col('birthday'), 'yyyyMMdd'))\\\n",
    "            \n",
    "customer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+---+---+\n",
      "|cus_type_cus_group|普通客户| 白金| 钻石|\n",
      "+------------------+----+---+---+\n",
      "|                个人|9932|289| 45|\n",
      "|                机构|  37|  0|  0|\n",
      "+------------------+----+---+---+\n",
      "\n",
      "+------------------+-------------------+\n",
      "|cus_type_freqItems|cus_group_freqItems|\n",
      "+------------------+-------------------+\n",
      "|          [个人, 机构]|     [钻石, 普通客户, 白金]|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cus_group</th>\n",
       "      <th>cus_type</th>\n",
       "      <th>女</th>\n",
       "      <th>男</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>普通客户</td>\n",
       "      <td>机构</td>\n",
       "      <td>-2034.837838</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>钻石</td>\n",
       "      <td>个人</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>55.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>普通客户</td>\n",
       "      <td>个人</td>\n",
       "      <td>48.623281</td>\n",
       "      <td>44.321323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>白金</td>\n",
       "      <td>个人</td>\n",
       "      <td>57.449275</td>\n",
       "      <td>49.536585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cus_group cus_type            女          男\n",
       "0      普通客户       机构 -2034.837838   1.000000\n",
       "1        钻石       个人    51.285714  55.294118\n",
       "2      普通客户       个人    48.623281  44.321323\n",
       "3        白金       个人    57.449275  49.536585"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉表\n",
    "customer.crosstab('cus_type','cus_group').show()\n",
    "# 类似unique\n",
    "customer.freqItems(['cus_type','cus_group']).show()\n",
    "# 按列运算，复制到新列\n",
    "customer = customer.withColumn('birth',customer.birthday + 10)\n",
    "# 透视表\n",
    "customer.groupBy('cus_group','cus_type').pivot(\"gender\").mean(\"age\").na.fill({'男':1}).toPandas()\n",
    "# cube 多维度观察数据 - cube的结果很奇怪\n",
    "# 和groupby后的结果是不同的\n",
    "customer.cube('cus_group',col('cus_type')).count().toPandas()\n",
    "\n",
    "customer.groupBy('cus_group','cus_type').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>cus_type</th>\n",
       "      <th>cus_group</th>\n",
       "      <th>certification</th>\n",
       "      <th>gender</th>\n",
       "      <th>risk</th>\n",
       "      <th>birthday</th>\n",
       "      <th>manager</th>\n",
       "      <th>birth</th>\n",
       "      <th>years</th>\n",
       "      <th>month</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000162168</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>女</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19560919</td>\n",
       "      <td>6000493</td>\n",
       "      <td>19560929</td>\n",
       "      <td>1956</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000162203</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>男</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19820630</td>\n",
       "      <td>haha</td>\n",
       "      <td>19820640</td>\n",
       "      <td>1982</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000190754</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>女</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19551211</td>\n",
       "      <td>haha</td>\n",
       "      <td>19551221</td>\n",
       "      <td>1955</td>\n",
       "      <td>12</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000190759</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>男</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19550529</td>\n",
       "      <td>haha</td>\n",
       "      <td>19550539</td>\n",
       "      <td>1955</td>\n",
       "      <td>5</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000190779</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>女</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19831222</td>\n",
       "      <td>haha</td>\n",
       "      <td>19831232</td>\n",
       "      <td>1983</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account cus_type cus_group certification gender risk  birthday  manager  \\\n",
       "0  1000162168       个人      普通客户           身份证      女  保守型  19560919  6000493   \n",
       "1  1000162203       个人      普通客户           身份证      男  保守型  19820630     haha   \n",
       "2  1000190754       个人      普通客户           身份证      女  保守型  19551211     haha   \n",
       "3  1000190759       个人      普通客户           身份证      男  保守型  19550529     haha   \n",
       "4  1000190779       个人      普通客户           身份证      女  保守型  19831222     haha   \n",
       "\n",
       "      birth  years  month  age  \n",
       "0  19560929   1956      9   61  \n",
       "1  19820640   1982      6   35  \n",
       "2  19551221   1955     12   62  \n",
       "3  19550539   1955      5   62  \n",
       "4  19831232   1983     12   34  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe 基本是只针对连续值\n",
    "customer.describe(['age','cus_group']).toPandas()\n",
    "# distinct前必须是df\n",
    "customer.select('age').distinct().toPandas()\n",
    "# drop\n",
    "customer.drop(col('manager')).toPandas()\n",
    "# fillna - 这里的貌似是''，没填补，用replace  - 这里是一个空格\n",
    "customer.na.fill({'manager':'haha'}).toPandas().head()\n",
    "customer.replace(' ','haha','manager').toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+----------+-------+---------+---------+--------+\n",
      "|   account|trans_id|trans_name|trans_date|prod_id|prod_name|trans_way|  amount|\n",
      "+----------+--------+----------+----------+-------+---------+---------+--------+\n",
      "|1000584205|  100200|    基金产品购买|  20161107| 000289| 鹏华丰泰定开债A|     网上银行|100000.0|\n",
      "|1000869570|  100200|    基金产品购买|  20161107| 000289| 鹏华丰泰定开债A|     网上银行| 50000.0|\n",
      "+----------+--------+----------+----------+-------+---------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "34909\n",
      "17666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤条件\n",
    "judge = ((func.col('account') == '1000599845') | (func.col('prod_id') == '000289'))\n",
    "trans.filter(judge).show(2)\n",
    "\n",
    "# first\n",
    "trans.first()\n",
    "\n",
    "# foreach\n",
    "# trans.account.foreach(lambda x:print x)\n",
    "\n",
    "# sample ， sampleBy(without replacement) - 按列按值抽样\n",
    "print trans.count()\n",
    "print trans.sample(False, fraction= 0.5,seed = 10).count()\n",
    "trans.sampleBy(col = 'trans_date',fractions = {'20161107':0.1,'20161017':0.5}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164097\n",
      "DataFrame[account: string, prod_id: string, count: bigint]\n",
      "34909\n"
     ]
    }
   ],
   "source": [
    "trans2 = trans\n",
    "oldColumns  = trans.columns\n",
    "newColumns = ['account','trans_id','trans_name','trans_date','prod_id','prod_name','trans_way','amount']\n",
    "newColumns = [name + '_1' for name in newColumns]\n",
    "trans2 = reduce(lambda trans, idx : trans.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)),trans)\n",
    "# join 看看\n",
    "cond =(trans.account == trans2.account_1) & (trans.prod_id == trans2.prod_id_1)\n",
    "print trans.join(trans2, cond , 'left').count()\n",
    "print trans.count()\n",
    "# trans.join(trans, cond , 'left').toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------+\n",
      "|row_number() OVER (PARTITION BY account ORDER BY amount ASC NULLS FIRST UnspecifiedFrame)|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "|                                                                                        1|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window类下面的方法加上functions下的方法，基本可以实现SQL中的很多需要的方法\n",
    "\n",
    "from pyspark.sql import Window\n",
    "window = Window.partitionBy('account').orderBy('amount')\n",
    "# .rangeBetween(0,-1)\n",
    "trans.select(row_number().over(window)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: string (nullable = true)\n",
      " |-- trans_id: string (nullable = true)\n",
      " |-- trans_name: string (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      " |-- trans_way: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      "\n",
      "+----------+------+------+--------+------+---------+----+-----+\n",
      "|       _c0|   _c1|   _c2|     _c3|   _c4|      _c5| _c6|  _c7|\n",
      "+----------+------+------+--------+------+---------+----+-----+\n",
      "|1000000132|200208|定期定额处理|20161017|162212|   泰达红利先锋|手机银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161117|162212|   泰达红利先锋|手机银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161219|162212|   泰达红利先锋|手机银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161107|001180|广发医药卫生联接A|网上银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161207|001180|广发医药卫生联接A|网上银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161024|001150|融通互联网传媒前端|网上银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161107|001150|融通互联网传媒前端|网上银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161121|001150|融通互联网传媒前端|网上银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161205|001150|融通互联网传媒前端|网上银行|500.0|\n",
      "|1000000132|200208|定期定额处理|20161219|001150|融通互联网传媒前端|网上银行|500.0|\n",
      "+----------+------+------+--------+------+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 排序 - 多种， orderBy是全量排序，sort是partition\n",
    "# persist()持久化后，不需要每次进行计算\n",
    "trans_order_limit = trans.orderBy(['account', 'prod_id'],ascending = [1, 0]).limit(10).persist()\n",
    "trans_order_limit.printSchema()\n",
    "# mode ， header 。。。\n",
    "trans_order_limit.write.csv(path + 'try.csv')\n",
    "trans_order_limit.write.format('csv').save(path + 'try2.csv')\n",
    "trans_order_limit.unpersist()\n",
    "# 再读\n",
    "aa = spark.read.csv(path + 'try.csv')\n",
    "aa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+\n",
      "|   account|cus_type|cus_group|certification|gender|risk|birthday|manager|years|month| age|binarized_age|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+\n",
      "|1000162168|      个人|     普通客户|          身份证|     女| 保守型|19560919|6000493| 1956|    9|61.0|          1.0|\n",
      "|1000162203|      个人|     普通客户|          身份证|     男| 保守型|19820630|       | 1982|    6|35.0|          1.0|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import *\n",
    "# 二值化 - inputCol必须转换为double类型 , outputCol必须没有过这一列\n",
    "customer = customer.withColumn('age',col('age').cast('double'))\n",
    "binarizer = Binarizer(threshold=30, inputCol=\"age\",outputCol=\"binarized_age\")\n",
    "customer = binarizer.transform(customer)\n",
    "customer.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+------------+\n",
      "|   account|cus_type|cus_group|certification|gender|risk|birthday|manager|years|month| age|binarized_age|bucketed_age|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+------------+\n",
      "|1000162168|      个人|     普通客户|          身份证|     女| 保守型|19560919|6000493| 1956|    9|61.0|          1.0|         5.0|\n",
      "|1000162203|      个人|     普通客户|          身份证|     男| 保守型|19820630|       | 1982|    6|35.0|          1.0|         3.0|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 连续值分箱 - 从0开始  - inputCol必须转换为double类型 , outputCol必须没有过这一列\n",
    "customer = customer.withColumn('age',col('age').cast('double'))\n",
    "split_list = [-float(\"inf\"), 0,20,30,40,60,float(\"inf\")]\n",
    "buckets = Bucketizer(splits = split_list, inputCol=\"age\", outputCol=\"bucketed_age\")\n",
    "customer = buckets.transform(customer)\n",
    "customer.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f098ac3f97eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mohEncoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'risk_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'risk_Vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mohEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcustomer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# customer.select(col('cus_type')).distinct().collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "# StringIndexer - LabelEncoder 先fit 再transform\n",
    "# 缺失值填充\n",
    "customer = customer.na.fill({'risk':'haha'})\n",
    "stringIndex = StringIndexer(inputCol='risk', outputCol='risk_index')\n",
    "customer = stringIndex.fit(customer).transform(customer)\n",
    "\n",
    "# customer.select(col('cus_type')).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+------------+----------+-------------+\n",
      "|   account|cus_type|cus_group|certification|gender|risk|birthday|manager|years|month| age|binarized_age|bucketed_age|risk_index|     risk_Vec|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+------------+----------+-------------+\n",
      "|1000162168|      个人|     普通客户|          身份证|     女| 保守型|19560919|6000493| 1956|    9|61.0|          1.0|         5.0|       0.0|(3,[0],[1.0])|\n",
      "|1000162203|      个人|     普通客户|          身份证|     男| 保守型|19820630|       | 1982|    6|35.0|          1.0|         3.0|       0.0|(3,[0],[1.0])|\n",
      "+----------+--------+---------+-------------+------+----+--------+-------+-----+-----+----+-------------+------------+----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder\n",
    "customer = customer.drop(col('risk_Vec'))\n",
    "ohEncoder = OneHotEncoder(inputCol='risk_index', outputCol='risk_Vec')\n",
    "customer = ohEncoder.transform(customer)\n",
    "customer.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: Column amount must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d6b0ea426e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'amount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'amountScaler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithMean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithStd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscalerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: Column amount must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'"
     ]
    }
   ],
   "source": [
    "# StandardScaler  -  貌似输入列得是个matrix\n",
    "trans = trans.na.fill({'amount':0}).withColumn('amount',col('amount').cast('double'))\n",
    "scaler = StandardScaler(inputCol = 'amount', outputCol='amountScaler', withMean=False, withStd=True)\n",
    "scalerModel =  scaler.fit(trans)\n",
    "trans = scalerModel.transform(trans)\n",
    "trans.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: Column amount must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a5c9f7486875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'amount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'amountScaler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscalerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: Column amount must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler -  貌似输入列得是个matrix\n",
    "trans = trans.na.fill({'amount':0}).withColumn('amount',col('amount').cast('double'))\n",
    "scaler = MinMaxScaler(inputCol = 'amount', outputCol='amountScaler')\n",
    "scalerModel =  scaler.fit(trans)\n",
    "trans = scalerModel.transform(trans)\n",
    "trans.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+----------+-------+---------+---------+------+--------+\n",
      "|   account|trans_id|trans_name|trans_date|prod_id|prod_name|trans_way|amount|     try|\n",
      "+----------+--------+----------+----------+-------+---------+---------+------+--------+\n",
      "|1003089018|  100541|    智能定投开通|  20161017| 020003|   国泰金龙行业|     网上银行|1000.0|100641.0|\n",
      "|1001172983|  100003|    银行账户变更|  20161017|       |     null|     网上银行|   0.0|100103.0|\n",
      "+----------+--------+----------+----------+-------+---------+---------+------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+--------+----------+----------+-------+---------+---------+------+\n",
      "|   account|trans_id|trans_name|trans_date|prod_id|prod_name|trans_way|  try2|\n",
      "+----------+--------+----------+----------+-------+---------+---------+------+\n",
      "|1003089018|  100541|    智能定投开通|  20161017| 020003|   国泰金龙行业|     网上银行|1000.0|\n",
      "|1001172983|  100003|    银行账户变更|  20161017|       |     null|     网上银行|   0.0|\n",
      "+----------+--------+----------+----------+-------+---------+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 添加列，重命名一个列\n",
    "trans.withColumn('try',trans.trans_id + 100).show(2)\n",
    "trans.withColumnRenamed('amount','try2').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20161017"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trans.select(trans.trans_date).rdd.min()\n",
    "# trans.agg(func.max('trans_date'),func.min('trans_date')).collect()\n",
    "int((trans.agg({'trans_date':'max','trans_date':'min'}).collect())[0]['min(trans_date)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans.select(func.date_format('trans_date','MM-dd-yyy').alias('date')).show()\n",
    "?trans.cube('account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans2 = trans.select(trans.account.alias('acc'),trans.trans_date.alias('dd'))\n",
    "new_trans= trans.join(trans2,trans.account == trans2.acc, \"inner\")\n",
    "sqlCtx.registerDataFrameAsTable(new_trans,'new_trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串转时间\n",
    "times = spark.sql(\"select account, (from_unixtime(unix_timestamp(trans_date,'yyyyMMdd'))) date1,(from_unixtime(unix_timestamp(dd,'yyyyMMdd'))) date2 from new_trans\")\n",
    "sqlCtx.registerDataFrameAsTable(times,'times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|   account|  dd|\n",
      "+----------+----+\n",
      "|1000493981|null|\n",
      "|1000017846|null|\n",
      "|1000017846|null|\n",
      "|1000017846|null|\n",
      "|1001864565|null|\n",
      "|1001864565|null|\n",
      "|1003068982|null|\n",
      "|1003068982|null|\n",
      "|1001864565|null|\n",
      "|1001864565|null|\n",
      "|1000000253|null|\n",
      "|1000000253|null|\n",
      "|1001088222|null|\n",
      "|1001225424|null|\n",
      "|1001920928|null|\n",
      "|1000000253|null|\n",
      "|1000000253|null|\n",
      "|1001298654|null|\n",
      "|1001298654|null|\n",
      "|1000017846|null|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select account , (date1 - date2) dd from times where date1 > date2 order by dd desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_api() takes exactly 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-0bb2f69fcdfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'account'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trans_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans_way'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prod_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _api() takes exactly 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "trans.groupBy('account').pivot('trans_name',['trans_way']).count('prod_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {20: 2,\n",
       "             30: 1,\n",
       "             50: 6,\n",
       "             60: 1,\n",
       "             80: 7,\n",
       "             100: 6712,\n",
       "             120: 6,\n",
       "             125: 4,\n",
       "             150: 29,\n",
       "             160: 1,\n",
       "             200: 2677,\n",
       "             250: 110,\n",
       "             300: 4000,\n",
       "             313: 1,\n",
       "             314: 1,\n",
       "             340: 8,\n",
       "             348: 2,\n",
       "             350: 18,\n",
       "             360: 1,\n",
       "             365: 1,\n",
       "             398: 2,\n",
       "             400: 6832,\n",
       "             401: 4,\n",
       "             403: 8,\n",
       "             420: 1,\n",
       "             450: 12,\n",
       "             470: 3,\n",
       "             499: 1,\n",
       "             500: 7178,\n",
       "             501: 1,\n",
       "             502: 1,\n",
       "             503: 8,\n",
       "             505: 6,\n",
       "             520: 7,\n",
       "             550: 1,\n",
       "             560: 1,\n",
       "             600: 477,\n",
       "             634: 1,\n",
       "             690: 1,\n",
       "             700: 61,\n",
       "             750: 2,\n",
       "             777: 1,\n",
       "             800: 373,\n",
       "             900: 27,\n",
       "             1000: 2412,\n",
       "             1001: 5,\n",
       "             1008: 3,\n",
       "             1010: 3,\n",
       "             1024: 1,\n",
       "             1060: 1,\n",
       "             1100: 4,\n",
       "             1200: 37,\n",
       "             1250: 16,\n",
       "             1300: 5,\n",
       "             1370: 1,\n",
       "             1400: 2,\n",
       "             1500: 127,\n",
       "             1502: 1,\n",
       "             1600: 20,\n",
       "             1630: 1,\n",
       "             1700: 2,\n",
       "             1721: 1,\n",
       "             1728: 2,\n",
       "             1789: 1,\n",
       "             1800: 12,\n",
       "             2000: 560,\n",
       "             2014: 3,\n",
       "             2017: 52,\n",
       "             2020: 2,\n",
       "             2050: 1,\n",
       "             2100: 15,\n",
       "             2137: 1,\n",
       "             2200: 2,\n",
       "             2300: 1,\n",
       "             2339: 1,\n",
       "             2340: 1,\n",
       "             2348: 3,\n",
       "             2480: 1,\n",
       "             2500: 60,\n",
       "             2600: 1,\n",
       "             2700: 1,\n",
       "             2800: 5,\n",
       "             3000: 210,\n",
       "             3100: 1,\n",
       "             3200: 3,\n",
       "             3500: 2,\n",
       "             3800: 4,\n",
       "             3900: 1,\n",
       "             4000: 37,\n",
       "             4300: 1,\n",
       "             4500: 6,\n",
       "             4600: 1,\n",
       "             4790: 1,\n",
       "             4880: 1,\n",
       "             4898: 1,\n",
       "             5000: 208,\n",
       "             5038: 1,\n",
       "             5400: 1,\n",
       "             5500: 2,\n",
       "             5800: 1,\n",
       "             6000: 16,\n",
       "             6500: 1,\n",
       "             6800: 1,\n",
       "             7000: 8,\n",
       "             7977: 1,\n",
       "             7983: 1,\n",
       "             8000: 21,\n",
       "             8200: 1,\n",
       "             8463: 1,\n",
       "             8600: 1,\n",
       "             8828: 1,\n",
       "             8940: 1,\n",
       "             9000: 5,\n",
       "             9500: 1,\n",
       "             10000: 288,\n",
       "             10007: 1,\n",
       "             11000: 4,\n",
       "             12000: 6,\n",
       "             13000: 5,\n",
       "             13700: 2,\n",
       "             13800: 1,\n",
       "             14000: 3,\n",
       "             14300: 1,\n",
       "             14400: 1,\n",
       "             15000: 24,\n",
       "             15203: 1,\n",
       "             16000: 6,\n",
       "             16800: 1,\n",
       "             17000: 6,\n",
       "             18000: 5,\n",
       "             19000: 1,\n",
       "             20000: 168,\n",
       "             21597: 1,\n",
       "             22000: 2,\n",
       "             22400: 1,\n",
       "             23000: 3,\n",
       "             23700: 1,\n",
       "             24000: 1,\n",
       "             25000: 6,\n",
       "             25997: 2,\n",
       "             27100: 1,\n",
       "             28000: 3,\n",
       "             30000: 96,\n",
       "             30200: 1,\n",
       "             31000: 1,\n",
       "             32000: 2,\n",
       "             33000: 2,\n",
       "             34000: 1,\n",
       "             35000: 11,\n",
       "             35500: 1,\n",
       "             36000: 1,\n",
       "             38100: 1,\n",
       "             39000: 2,\n",
       "             40000: 54,\n",
       "             40001: 2,\n",
       "             43000: 1,\n",
       "             43480: 1,\n",
       "             44000: 1,\n",
       "             45000: 6,\n",
       "             46000: 3,\n",
       "             48000: 2,\n",
       "             49990: 1,\n",
       "             50000: 425,\n",
       "             50590: 1,\n",
       "             50700: 1,\n",
       "             51000: 4,\n",
       "             52000: 2,\n",
       "             52300: 1,\n",
       "             53000: 6,\n",
       "             54000: 1,\n",
       "             55000: 17,\n",
       "             55478: 1,\n",
       "             56000: 5,\n",
       "             56867: 1,\n",
       "             56900: 1,\n",
       "             57000: 2,\n",
       "             58000: 4,\n",
       "             59500: 1,\n",
       "             60000: 77,\n",
       "             61000: 1,\n",
       "             62000: 2,\n",
       "             64500: 1,\n",
       "             65000: 7,\n",
       "             68000: 3,\n",
       "             70000: 49,\n",
       "             71000: 1,\n",
       "             71320: 1,\n",
       "             74000: 1,\n",
       "             75000: 3,\n",
       "             76000: 2,\n",
       "             78000: 1,\n",
       "             79000: 1,\n",
       "             80000: 55,\n",
       "             81000: 1,\n",
       "             82000: 2,\n",
       "             84000: 1,\n",
       "             85000: 1,\n",
       "             86000: 1,\n",
       "             89000: 1,\n",
       "             90000: 17,\n",
       "             91000: 1,\n",
       "             92000: 1,\n",
       "             93000: 1,\n",
       "             97000: 1,\n",
       "             99000: 1,\n",
       "             100000: 283,\n",
       "             100006: 1,\n",
       "             101000: 1,\n",
       "             102000: 1,\n",
       "             103000: 2,\n",
       "             103500: 1,\n",
       "             104000: 2,\n",
       "             104234: 1,\n",
       "             105000: 4,\n",
       "             107000: 2,\n",
       "             110000: 18,\n",
       "             113000: 1,\n",
       "             114222: 1,\n",
       "             117000: 1,\n",
       "             119000: 1,\n",
       "             120000: 25,\n",
       "             123000: 1,\n",
       "             125000: 2,\n",
       "             125997: 1,\n",
       "             127000: 1,\n",
       "             129000: 1,\n",
       "             130000: 17,\n",
       "             132000: 1,\n",
       "             140000: 9,\n",
       "             143200: 1,\n",
       "             150000: 40,\n",
       "             154000: 1,\n",
       "             155000: 1,\n",
       "             157000: 1,\n",
       "             160000: 12,\n",
       "             170000: 6,\n",
       "             178000: 1,\n",
       "             180000: 12,\n",
       "             181600: 1,\n",
       "             185000: 1,\n",
       "             187000: 2,\n",
       "             190000: 5,\n",
       "             191000: 1,\n",
       "             200000: 102,\n",
       "             200630: 3,\n",
       "             205920: 1,\n",
       "             208000: 1,\n",
       "             210000: 5,\n",
       "             213000: 1,\n",
       "             219000: 1,\n",
       "             220000: 3,\n",
       "             225000: 1,\n",
       "             229000: 1,\n",
       "             230000: 12,\n",
       "             231326: 1,\n",
       "             243000: 1,\n",
       "             250000: 15,\n",
       "             258000: 1,\n",
       "             260000: 6,\n",
       "             270000: 1,\n",
       "             280000: 5,\n",
       "             290000: 2,\n",
       "             296000: 1,\n",
       "             300000: 31,\n",
       "             306000: 1,\n",
       "             307000: 1,\n",
       "             310000: 1,\n",
       "             316000: 1,\n",
       "             320000: 2,\n",
       "             330000: 2,\n",
       "             340000: 1,\n",
       "             350000: 2,\n",
       "             360000: 1,\n",
       "             370000: 1,\n",
       "             376000: 1,\n",
       "             379000: 2,\n",
       "             380000: 4,\n",
       "             386000: 1,\n",
       "             388000: 1,\n",
       "             390000: 4,\n",
       "             395000: 1,\n",
       "             397000: 1,\n",
       "             400000: 14,\n",
       "             410000: 2,\n",
       "             420000: 1,\n",
       "             430000: 3,\n",
       "             450000: 4,\n",
       "             460000: 1,\n",
       "             470000: 1,\n",
       "             490000: 1,\n",
       "             498000: 1,\n",
       "             500000: 26,\n",
       "             506000: 1,\n",
       "             510000: 2,\n",
       "             519000: 1,\n",
       "             520000: 1,\n",
       "             528000: 1,\n",
       "             540000: 1,\n",
       "             550000: 1,\n",
       "             560000: 1,\n",
       "             600000: 7,\n",
       "             610000: 1,\n",
       "             620000: 1,\n",
       "             650000: 3,\n",
       "             680000: 1,\n",
       "             700000: 3,\n",
       "             800000: 3,\n",
       "             880000: 2,\n",
       "             900000: 1,\n",
       "             995000: 2,\n",
       "             1000000: 22,\n",
       "             1100000: 2,\n",
       "             1200000: 1,\n",
       "             1280000: 1,\n",
       "             1295000: 1,\n",
       "             1350000: 2,\n",
       "             1360000: 2,\n",
       "             1600000: 1,\n",
       "             1650000: 1,\n",
       "             1800000: 1,\n",
       "             1900000: 2,\n",
       "             1990000: 1,\n",
       "             2000000: 10,\n",
       "             2400000: 1,\n",
       "             3000000: 2,\n",
       "             3133000: 1,\n",
       "             6810000: 1,\n",
       "             15001537: 1})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = trans.rdd.map(lambda fields:fields.amount).map(lambda x:int(x))\n",
    "# 返回了个defaultdict\n",
    "\n",
    "tr.filter(lambda x: x > 10).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+\n",
      "|       客户号|  产品代码|    投资成本|\n",
      "+----------+------+--------+\n",
      "|1003281382|   100|  1500.0|\n",
      "|1000744878|100016|     0.0|\n",
      "|1000295034|100018| 15000.0|\n",
      "|1001601251|100018|800000.0|\n",
      "|1001244454|100020|  1500.0|\n",
      "|1000004109|100021|     0.0|\n",
      "|1002833265|100022|     0.0|\n",
      "|1000977819|100022|     0.0|\n",
      "|1002410662|100022|     0.0|\n",
      "|1000706323|100022|     0.0|\n",
      "|1001162745|100022|     0.0|\n",
      "|1001292053|100022|     0.0|\n",
      "|1002068170|100022| 10000.0|\n",
      "|1002803375|100022|  1200.0|\n",
      "|1001731895|100022|  1500.0|\n",
      "|1000630433|100022| 15000.0|\n",
      "|1001882209|100022|  2000.0|\n",
      "|1001875108|100022|  2100.0|\n",
      "|1002000342|100022|  2400.0|\n",
      "|1000275783|100022| 28000.0|\n",
      "+----------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+------+--------+\n",
      "|       客户号|  产品代码|    投资成本|\n",
      "+----------+------+--------+\n",
      "|1003281382|   100|  1500.0|\n",
      "|1000744878|100016|     0.0|\n",
      "|1000295034|100018| 15000.0|\n",
      "|1001601251|100018|800000.0|\n",
      "|1001244454|100020|  1500.0|\n",
      "|1000004109|100021|     0.0|\n",
      "|1002833265|100022|     0.0|\n",
      "|1000977819|100022|     0.0|\n",
      "|1002410662|100022|     0.0|\n",
      "|1000706323|100022|     0.0|\n",
      "|1001162745|100022|     0.0|\n",
      "|1001292053|100022|     0.0|\n",
      "|1002068170|100022| 10000.0|\n",
      "|1002803375|100022|  1200.0|\n",
      "|1001731895|100022|  1500.0|\n",
      "|1000630433|100022| 15000.0|\n",
      "|1001882209|100022|  2000.0|\n",
      "|1001875108|100022|  2100.0|\n",
      "|1002000342|100022|  2400.0|\n",
      "|1000275783|100022| 28000.0|\n",
      "+----------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 排序， 还有orderby\n",
    "cost.sort([u'产品代码',u'投资成本'],ascending = [True,True]).show()\n",
    "cost.orderBy([u'产品代码',u'投资成本'],ascending = [True,True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择莫一列，重命名列\n",
    "new_cost = cost.select(cost[u'产品代码'].alias('prod_id'), (cost[u'投资成本'] + 10).alias('age'))\n",
    "new_cost.registerTempTable('new_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "?new_cost.alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|prod_id|           avg_age|\n",
      "+-------+------------------+\n",
      "| 100018|          407510.0|\n",
      "| 690012|          380010.0|\n",
      "|    747|        356085.155|\n",
      "|    177| 355343.3333333333|\n",
      "|  40018|          347010.0|\n",
      "|   1763|          332010.0|\n",
      "|   2197|          275010.0|\n",
      "|  20022|215849.91999999998|\n",
      "| 240008|          207010.0|\n",
      "|   1534|          200010.0|\n",
      "| 270004|172605.33333333334|\n",
      "| 110050|         169841.82|\n",
      "|    222|157198.88888888888|\n",
      "|    647|         156096.75|\n",
      "|    221|154901.30434782608|\n",
      "|   2946|153343.33333333334|\n",
      "|    235|149535.38461538462|\n",
      "| 110007|122937.27272727272|\n",
      "|   1917|          120460.0|\n",
      "|   1816|          120010.0|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_cost.groupBy('prod_id').agg(func.avg(\"age\").alias('avg_age')).orderBy('avg_age',ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "?func.bin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |       cc|       true|\n",
      "|        |     cost|       true|\n",
      "|        | customer|       true|\n",
      "|        | new_cost|       true|\n",
      "|        |  product|       true|\n",
      "|        |     test|       true|\n",
      "|        |    trans|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: key cannot be null'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-f71fab1ce6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python Spark SQL basic example\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.some.config.option\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"some-value\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: key cannot be null'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+--------+------+--------+----+------+\n",
      "|      客户编号|  交易代码|  交易名称|    交易日期|  产品代码|    产品名称|交易渠道|  交易金额|\n",
      "+----------+------+------+--------+------+--------+----+------+\n",
      "|1003089018|100541|智能定投开通|20161017|020003|  国泰金龙行业|网上银行|1000.0|\n",
      "|1001172983|100003|银行账户变更|20161017|      |    null|网上银行|   0.0|\n",
      "|1001232639|100208|定期定额开通|20161017|100056|  富国低碳环保|网上银行| 100.0|\n",
      "|1001232639|100208|定期定额开通|20161017|001616|  嘉实环保低碳|网上银行| 100.0|\n",
      "|1003275159|100541|智能定投开通|20161017|000136|民生策略精选混合|网上银行| 400.0|\n",
      "|1003276023|100208|定期定额开通|20161017|340007|  兴全社会责任|网上银行| 100.0|\n",
      "|1003276005|100208|定期定额开通|20161017|340007|  兴全社会责任|网上银行| 100.0|\n",
      "|1000068852|100541|智能定投开通|20161017|320011|   诺安中小盘|手机银行| 600.0|\n",
      "|1002605841|100208|定期定额开通|20161017|020003|  国泰金龙行业|网上银行| 500.0|\n",
      "|1003276582|100208|定期定额开通|20161017|000136|民生策略精选混合|网上银行|1000.0|\n",
      "|1003267717|100208|定期定额开通|20161017|100056|  富国低碳环保|网上银行| 100.0|\n",
      "|1003276954|100541|智能定投开通|20161017|000136|民生策略精选混合|网上银行| 400.0|\n",
      "|1002978893|200208|定期定额处理|20161017|000452|  南方医药保健|网上银行| 100.0|\n",
      "|1002454532|200208|定期定额处理|20161017|070001|  嘉实成长收益|手机银行| 100.0|\n",
      "|1003222133|200208|定期定额处理|20161017|690002|   民生强债A|网上银行| 100.0|\n",
      "|1003140211|200208|定期定额处理|20161017|320011|   诺安中小盘|网上银行| 100.0|\n",
      "|1002650269|200208|定期定额处理|20161017|340007|  兴全社会责任|网上银行| 100.0|\n",
      "|1001202406|200208|定期定额处理|20161017|001417| 汇添富医疗服务|网上银行| 500.0|\n",
      "|1002820000|200208|定期定额处理|20161017|070001|  嘉实成长收益|网上银行| 100.0|\n",
      "|1003215220|200208|定期定额处理|20161017|000561| 南方启元债券A|网上银行| 300.0|\n",
      "+----------+------+------+--------+------+--------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trans.createGlobalTempView('trans_new')\n",
    "spark.sql(\"SELECT * FROM global_temp.trans_new\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "age = new_cost.rdd.map(lambda p: \"Name: \" + str(p['age'])).collect()\n",
    "prod = new_cost.rdd.map(lambda p: \"Name: \" + p.prod_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: 4510.0'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.master', u'local[*]'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'PySparkShell')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'word', 'hi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['hello', 'word'], ['hi']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"hello word\",\"hi\"])\n",
    "words = lines.flatMap(lambda line:line.split(\" \"))\n",
    "print words.collect()\n",
    "words = lines.map(lambda line:line.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "?lines.aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分布式，comb是把各个分去再合并\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(account=u'1000162168', cus_type=u'\\u4e2a\\u4eba', cus_group=u'\\u666e\\u901a\\u5ba2\\u6237', certification=u'\\u8eab\\u4efd\\u8bc1', gender=u'\\u5973', risk=u'\\u4fdd\\u5b88\\u578b', birthday=u'19560919', manager=u'6000493'),\n",
       " Row(account=u'1000162203', cus_type=u'\\u4e2a\\u4eba', cus_group=u'\\u666e\\u901a\\u5ba2\\u6237', certification=u'\\u8eab\\u4efd\\u8bc1', gender=u'\\u7537', risk=u'\\u4fdd\\u5b88\\u578b', birthday=u'19820630', manager=u' '),\n",
       " Row(account=u'1000190754', cus_type=u'\\u4e2a\\u4eba', cus_group=u'\\u666e\\u901a\\u5ba2\\u6237', certification=u'\\u8eab\\u4efd\\u8bc1', gender=u'\\u5973', risk=u'\\u4fdd\\u5b88\\u578b', birthday=u'19551211', manager=u' '),\n",
       " Row(account=u'1000190759', cus_type=u'\\u4e2a\\u4eba', cus_group=u'\\u666e\\u901a\\u5ba2\\u6237', certification=u'\\u8eab\\u4efd\\u8bc1', gender=u'\\u7537', risk=u'\\u4fdd\\u5b88\\u578b', birthday=u'19550529', manager=u' '),\n",
       " Row(account=u'1000190779', cus_type=u'\\u4e2a\\u4eba', cus_group=u'\\u666e\\u901a\\u5ba2\\u6237', certification=u'\\u8eab\\u4efd\\u8bc1', gender=u'\\u5973', risk=u'\\u4fdd\\u5b88\\u578b', birthday=u'19831222', manager=u' ')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.rdd.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|   account|prod_id| amount|\n",
      "+----------+-------+-------+\n",
      "|1000000132|    942| 4500.0|\n",
      "|1000000132|   1180| 4500.0|\n",
      "|1000000132|    823|47000.0|\n",
      "|1000000132|   1150|24000.0|\n",
      "|1000000140| 470008|    0.0|\n",
      "|1000000161|  50026|  400.0|\n",
      "|1000000161|   2644|    0.0|\n",
      "|1000000161|   1743|10000.0|\n",
      "|1000000161|   2560|    0.0|\n",
      "|1000000161| 240020|  400.0|\n",
      "|1000000161|     83| 4000.0|\n",
      "|1000000161|   1125| 4000.0|\n",
      "|1000000167| 320015| 6000.0|\n",
      "|1000000167| 161655| 9000.0|\n",
      "|1000000170| 161654|22000.0|\n",
      "|1000000170| 270010|22000.0|\n",
      "|1000000182| 470009|10000.0|\n",
      "|1000000182|   1125| 2000.0|\n",
      "|1000000188| 470009|46000.0|\n",
      "|1000000188|  70022|    0.0|\n",
      "+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_prod = cost.rdd.map(lambda x:(x.account,x.prod_id))\n",
    "acc_amount = cost.rdd.map(lambda x:(x.account,x.amount))\n",
    "acc_prod_j = acc_prod.mapValues(lambda x: x + 'haha')\n",
    "acc_amount_j = acc_amount.mapValues(lambda x: x + 'heihei')\n",
    "a_p_j = acc_prod_j.join(acc_amount_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "?datetime.strptime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = sqlContext.createDataFrame(a_p_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>cus_type</th>\n",
       "      <th>cus_group</th>\n",
       "      <th>certification</th>\n",
       "      <th>gender</th>\n",
       "      <th>risk</th>\n",
       "      <th>birthday</th>\n",
       "      <th>manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000162168</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>女</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19560919</td>\n",
       "      <td>6000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000162203</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>男</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19820630</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000190754</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>女</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19551211</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000190759</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>男</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19550529</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000190779</td>\n",
       "      <td>个人</td>\n",
       "      <td>普通客户</td>\n",
       "      <td>身份证</td>\n",
       "      <td>女</td>\n",
       "      <td>保守型</td>\n",
       "      <td>19831222</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account cus_type cus_group certification gender risk  birthday  manager\n",
       "0  1000162168       个人      普通客户           身份证      女  保守型  19560919  6000493\n",
       "1  1000162203       个人      普通客户           身份证      男  保守型  19820630         \n",
       "2  1000190754       个人      普通客户           身份证      女  保守型  19551211         \n",
       "3  1000190759       个人      普通客户           身份证      男  保守型  19550529         \n",
       "4  1000190779       个人      普通客户           身份证      女  保守型  19831222         "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpd = customer.toPandas()\n",
    "cpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
